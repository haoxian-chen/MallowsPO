# MallowsPO
We include necessary components needed for implmenting MallowsPO in the trainer, modified from [trl](https://github.com/huggingface/trl) dpo trainer. MallowsPO can be implemented pretty easily by modifying from the common LLM RLHF libaries' implementations on DPO. A more detailed codebase, which takes MallowsPO as a special instance (contextual scaling), can be found at [RainbowPO](https://github.com/CapitalOne-Research/RainbowPO).

## ðŸ“œ Citation

If you find **MallowsPO** useful in your research, please consider citing our work! ðŸš€

### BibTeX
```bibtex
@article{chen2024mallows,
  title={Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions},
  author={Chen, Haoxian and Zhao, Hanyang and Lam, Henry and Yao, David and Tang, Wenpin},
  journal={arXiv preprint arXiv:2405.14953},
  year={2024}
}
